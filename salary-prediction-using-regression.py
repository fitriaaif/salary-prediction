# -*- coding: utf-8 -*-
"""Tim22B_Python Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HY-ahvekCzuDHVqI95hbV4jfVPZrYwV

#Project Title
Employee Salary Prediction in Industry IT Services and IT Consulting Based on Skills With Regression Method

(Prediksi Gaji Karyawan di Industri IT Services and IT Consulting Berdasarkan Keahlian Dengan Metode Regresi)
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Data Preprocessing"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import resample

# Memanggil dataset ke dalam project
data = pd.read_csv("./drive/MyDrive/Colab Notebooks/linkedIn_posting/lowongan_kerja.csv")
data

"""##Data Cleaning Lowongan Kerja (data_lowongan)"""

# Memanggil dataset ke dalam project
data_lowongan = pd.read_csv("./drive/MyDrive/Colab Notebooks/linkedIn_posting/lowongan_kerja.csv")
data_lowongan

# menghitung nilai yang hilang di setiap kolom
data_lowongan.isna().sum()

# Mengisi nilai yang hilang dalam kolom objek dengan 'unknown'
columns_to_fill = ['deskripsi', 'periode_pembayaran', 'url_pendaftaran',
                   'tingkat_pengalaman_terformat', 'deskripsi_keterampilan', 'domain_penyelenggaraan']
data_lowongan[columns_to_fill] = data_lowongan[columns_to_fill].fillna('unknown')

# Hasilnya akan menjadi dataframe 'df' yang telah diisi nilai yang hilang dengan 'unknown'
data_lowongan

# Mendapatkan nilai modus dari kolom yang diinginkan
modus_tipe_kompensasi = data_lowongan['tipe_kompensasi'].mode()[0]

# Mengisi nilai yang hilang dalam kolom 'lokasi' dengan nilai modus
data_lowongan['tipe_kompensasi'].fillna(modus_tipe_kompensasi, inplace=True)

# Mendapatkan nilai modus dari kolom yang diinginkan
modus_mata_uang = data_lowongan['mata_uang'].mode()[0]

# Mengisi nilai yang hilang dalam kolom 'lokasi' dengan nilai modus
data_lowongan['mata_uang'].fillna(modus_mata_uang, inplace=True)

# Menghapus baris yang memiliki nilai hilang di kolom 'id_perusahaan'
data_lowongan.dropna(subset=['id_perusahaan'], inplace=True)

# Mendapatkan nilai modus dari kolom 'diperbolehkan_jarak_jauh'
modus_diperbolehkan_jarak_jauh = data_lowongan['diperbolehkan_jarak_jauh'].mode()[0]

# Mengisi nilai yang hilang dalam kolom 'diperbolehkan_jarak_jauh' dengan nilai modus
data_lowongan['diperbolehkan_jarak_jauh'].fillna(modus_diperbolehkan_jarak_jauh, inplace=True)

# Mendapatkan nilai modus dari kolom 'tampilan'
modus_tampilan = data_lowongan['tampilan'].mode()[0]

# Mengisi nilai yang hilang dalam kolom 'tampilan' dengan nilai modus
data_lowongan['tampilan'].fillna(modus_tampilan, inplace=True)

# Mendapatkan nilai modus dari kolom 'lamaran'
modus_lamaran = data_lowongan['lamaran'].mode()[0]

# Mengisi nilai yang hilang dalam kolom 'lamaran' dengan nilai modus
data_lowongan['lamaran'].fillna(modus_lamaran, inplace=True)

# Menghitung nilai yang hilang di setiap kolom
data_lowongan.isna().sum()

data_lowongan.dropna(how='all', subset=['gaji_maksimal', 'gaji_tengah', 'gaji_minimal'], inplace=True)

# Mengisi nilai gaji_median yang Null dengan (gaji_max + gaji_min) / 2
data_lowongan['gaji_tengah'] = data_lowongan.apply(
    lambda row: (row['gaji_maksimal'] + row['gaji_minimal']) / 2 if pd.isnull(row['gaji_tengah']) else row['gaji_tengah'],
    axis=1
)

# Mengisi nilai gaji_max yang Null dengan gaji_median * 1.1
data_lowongan['gaji_maksimal'] = data_lowongan.apply(
    lambda row: row['gaji_tengah'] * 1.1 if pd.isnull(row['gaji_maksimal']) else row['gaji_maksimal'],
    axis=1
)

# Mengisi nilai gaji_min yang Null dengan gaji_median * 0.9
data_lowongan['gaji_minimal'] = data_lowongan.apply(
    lambda row: row['gaji_tengah'] * 0.9 if pd.isnull(row['gaji_minimal']) else row['gaji_minimal'],
    axis=1
)
data_lowongan

"""Case Kalau diantara fields gaji_max-min-med = ketiga fields tersebut gak ada datanya, Solusinya : bisa ngambil data gaji serupa berdasarkan skill, perusahaan & pengalaman kerja (Kalau gak pengen hapus data) **DONE**

Case kalau diantara fields gaji_max-min-med = 1 atau 2 fields gak ada datanya , solusinya :
- bisa ngisi data yang kosong pakai rumus (buat nyari nilai median = (gaji_max + gaji_min) /2 | (buat nyari gaji_min & gaji_max = median * 1.1 & median * 0.9) atau bisa sama ratain aja ngikutin nilai median **DONE**
- ngisi data yang kosong, dengan cara ngambil data gaji serupa berdasarkan skill, perusahaan & pengalaman kerja + sesuakian sama nilai data gaji yang ada (biar gak over rate)
"""

data_lowongan.isna().sum()

# Melihat baris yang memiliki data duplikat
duplicate_rows = data_lowongan[data_lowongan.duplicated()]

# Menampilkan baris yang memiliki data duplikat
print(duplicate_rows)

target = data_lowongan['jenis_pekerjaan']

# Hitung rasio kelas
class_ratio = target.value_counts() / len(target)

# Hitung rasio keburaman (oversampling)
obscurity_ratio = len(target[target == target.mode()[0]]) / len(target[target != target.mode()[0]])

# Hitung faktor ketidakseimbangan
imbalance_factor = len(target[target == target.mode()[0]]) / len(target[target != target.mode()[0]])

# Hitung Gini impurity
gini_impurity = 1 - sum((class_ratio) ** 2)

# Hitung entropi
entropy = -sum(class_ratio * np.log2(class_ratio))

print("Rasio Kelas:")
print(class_ratio)

print("\nRasio Keburaman (Obscurity Ratio):", obscurity_ratio)

print("\nFaktor Ketidakseimbangan (Imbalance Factor):", imbalance_factor)

print("\nGini Impurity:", gini_impurity)

print("\nEntropi:", entropy)

data_lowongan

"""## Data Cleaning Skill Pekerjaan (data_skill)"""

# Memanggil dataset kedalam projek
data_skill = pd.read_csv("./drive/MyDrive/Colab Notebooks/linkedIn_posting/detail_pekerjaan/skill_pekerjaan.csv")
data_skill

# Mengidentifikasi nilai yang hilang
missing_values = data_skill.isna()

# Menampilkan jumlah nilai yang hilang untuk setiap kolom
print(missing_values)

# Menghitung nilai yang hilang di setiap kolom
data_skill.isna().sum()

data_skill.id_pekerjaan.duplicated().sum()

data_skill.drop_duplicates(subset=['id_pekerjaan'], inplace=True)
data_skill.id_pekerjaan.duplicated().sum()

# Melihat baris yang memiliki data duplikat
duplicate_rows = data_skill[data_skill.duplicated()]

# Menampilkan baris yang memiliki data duplikat
print(duplicate_rows)

target = data_skill['id_pekerjaan']

# Hitung rasio kelas
class_ratio = target.value_counts() / len(target)

# Hitung rasio keburaman (oversampling)
obscurity_ratio = len(target[target == target.mode()[0]]) / len(target[target != target.mode()[0]])

# Hitung faktor ketidakseimbangan
imbalance_factor = len(target[target == target.mode()[0]]) / len(target[target != target.mode()[0]])

# Hitung Gini impurity
gini_impurity = 1 - sum((class_ratio) ** 2)

# Hitung entropi
entropy = -sum(class_ratio * np.log2(class_ratio))

print("Rasio Kelas:")
print(class_ratio)

print("\nRasio Keburaman (Obscurity Ratio):", obscurity_ratio)

print("\nFaktor Ketidakseimbangan (Imbalance Factor):", imbalance_factor)

print("\nGini Impurity:", gini_impurity)

print("\nEntropi:", entropy)

data_skill

"""##Data Cleaning Kemampuan (data_kemampuan)"""

# Memanggil dataset kedalam projek
data_kemampuan = pd.read_csv("./drive/MyDrive/Colab Notebooks/linkedIn_posting/mapping/kemampuan.csv")
data_kemampuan

# Mengidentifikasi nilai yang hilang
missing_values = data_kemampuan.isna()

# Menampilkan jumlah nilai yang hilang untuk setiap kolom
print(missing_values)

# Menghitung nilai yang hilang di setiap kolom
data_kemampuan.isna().sum()

data_kemampuan.duplicated().sum()

# Melihat baris yang memiliki data duplikat
duplicate_rows = data_kemampuan[data_kemampuan.duplicated()]

# Menampilkan baris yang memiliki data duplikat
print(duplicate_rows)

target = data_kemampuan['nama_kemampuan']

# Hitung rasio kelas
class_ratio = target.value_counts() / len(target)

# Hitung rasio keburaman (oversampling)
obscurity_ratio = len(target[target == target.mode()[0]]) / len(target[target != target.mode()[0]])

# Hitung faktor ketidakseimbangan
imbalance_factor = len(target[target == target.mode()[0]]) / len(target[target != target.mode()[0]])

# Hitung Gini impurity
gini_impurity = 1 - sum((class_ratio) ** 2)

# Hitung entropi
entropy = -sum(class_ratio * np.log2(class_ratio))

print("Rasio Kelas:")
print(class_ratio)
print("\nRasio Keburaman (Obscurity Ratio):", obscurity_ratio)
print("\nFaktor Ketidakseimbangan (Imbalance Factor):", imbalance_factor)
print("\nGini Impurity:", gini_impurity)
print("\nEntropi:", entropy)

data_kemampuan

"""##Data Cleaning Upah (df_upah)"""

# Memanggil dataset kedalam projek
df_upah = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/linkedIn_posting/detail_pekerjaan/upah.csv")
df_upah

# Mendeteksi missing Vallue
df_upah.info()

missing_values = df_upah.isna().sum()
missing_values

# Mengisi nilai yang hilang dengan menyimpan hasilnya kembali ke DataFrame
df_upah = df_upah.copy()  # pastikan memiliki DataFrame salinan jika diperlukan

df_upah['gaji_maksimal'] = df_upah['gaji_maksimal'].fillna(df_upah['gaji_maksimal'].mode())
df_upah['gaji_median'] = df_upah['gaji_median'].fillna(df_upah['gaji_median'].mode())
df_upah['gaji_minimal'] = df_upah['gaji_minimal'].fillna(df_upah['gaji_minimal'].mode())

missing_values = df_upah.isna().sum()
missing_values

# Menghapus baris yang memiliki nilai kosong diketiga kolom gaji
df_upah.dropna(how='all', subset=['gaji_maksimal', 'gaji_median', 'gaji_minimal'], inplace=True)

# Mengisi nilai gaji_median yang Null dengan (gaji_max + gaji_min) / 2
df_upah['gaji_median'] = df_upah.apply(
    lambda row: (row['gaji_maksimal'] + row['gaji_minimal']) / 2 if pd.isnull(row['gaji_median']) else row['gaji_median'],
    axis=1
)

# Mengisi nilai gaji_max yang Null dengan gaji_median * 1.1
df_upah['gaji_maksimal'] = df_upah.apply(
    lambda row: row['gaji_median'] * 1.1 if pd.isnull(row['gaji_maksimal']) else row['gaji_maksimal'],
    axis=1
)

# Mengisi nilai gaji_min yang Null dengan gaji_median * 0.9
df_upah['gaji_minimal'] = df_upah.apply(
    lambda row: row['gaji_median'] * 0.9 if pd.isnull(row['gaji_minimal']) else row['gaji_minimal'],
    axis=1
)
df_upah

# Asumsikan df_upah sudah ada dan memiliki kolom gaji_maksimal, gaji_median, gaji_minimal
# Buat plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_upah[['gaji_maksimal', 'gaji_median', 'gaji_minimal']])

# Tambahkan judul dan label sumbu
plt.title("Boxplot Gaji Maksimal, Median, dan Minimal")
plt.xlabel("Jenis Gaji")
plt.ylabel("Nilai Gaji")

# Tampilkan plot
plt.show()

# Menghitung berapa baris yg duplicate
duplicate_rows = df_upah.duplicated().sum()
print(duplicate_rows)

# Melihat isi data yg duplicate
duplicate_rows = df_upah[df_upah.duplicated()]
duplicate_rows.head(10)

df_upah

"""## Data Cleaning Industri"""

# Memanggil dataset kedalam projek
industri = pd.read_csv("/content/drive/My Drive/Buat Ngerjain Featured Engineering/industri.csv")
industri

# Memeriksa missing Vallue
industri.info()

industri.isna().sum()

industri['nama_industri'].fillna('Unknown',inplace=True)

industri.isna().sum()

# Menghitung berapa baris yg duplicate
duplicate = industri.id_industri.duplicated().sum()
print(duplicate)

industri.drop_duplicates(subset=['id_industri'], inplace=True)
industri.id_industri.duplicated().sum()

industri

"""## Pendefinisian ulang dataset"""

# Dataset lowongan_kerja.csv
df_lk = data_lowongan
df_lk

# Dataset skill_pekerjaan.csv
df_skill_pekerjaan = data_skill
df_skill_pekerjaan

df_industri_pekerjaan = pd.read_csv('/content/drive/My Drive/Buat Ngerjain Featured Engineering/industri_pekerjaan.csv')
df_industri_pekerjaan

# Dataset industri.csv
df_industri = industri
df_industri

# Memuat dataset kemampuan
df_kemampuan = data_kemampuan
df_kemampuan

# Dataset perusahaan.csv
df_perusahaan = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/linkedIn_posting/detail_perusahaan/perusahaan.csv') #'/content/drive/My Drive/Buat Ngerjain Featured Engineering/perusahaan.csv'
df_perusahaan

# Dataset upah.csv
df_gaji = df_upah
df_gaji

"""## Feature Extraction
Seleksi kolom dataset

### lowongan_kerja (df_lk_clean)
"""

df_lk

# Menghapus Kolom yang tidak digunakan
kolom_lk_dihapus = ['jenis_pekerjaan_terformat', 'lokasi', 'lamaran', 'diperbolehkan_jarak_jauh', 'tampilan', 'tipe_pendaftaran', 'mata_uang', 'tipe_kompensasi',	'waktu_kedaluwarsa', 'waktu_daftar', 'domain_penyelenggaraan', 'disponsori', 'url_posting_pekerjaan',	'url_pendaftaran']
df_lk_clean = df_lk.drop(columns=kolom_lk_dihapus) # Sementara id_pekerjaan & id_perusahaan tidak dihapus karena masih berguna untuk join data

df_lk_clean

"""### perusahaan (df_perusahaan_clean)"""

df_perusahaan

# Menghapus Kolom yang tidak digunakan
kolom_perusahaan_dihapus = ['deskripsi',	'ukuran_perusahaan',	'negara_bagian',	'negara',	'kota',	'kode_pos',	'alamat',	'url']
df_perusahaan_clean = df_perusahaan.drop(columns=kolom_perusahaan_dihapus) # Sementara id_pekerjaan & id_perusahaan tidak dihapus karena masih berguna untuk join data

df_perusahaan_clean

"""### upah (df_gaji_clean)"""

df_gaji

# Menghapus Kolom yang tidak digunakan
kolom_gaji_dihapus = ['id_gaji', 'mata_uang', 'jenis_kompensasi']
df_gaji_clean = df_gaji.drop(columns=kolom_gaji_dihapus)

df_gaji_clean

"""## Feature Engineering

### lowongan_kerja.csv dan skill_pekerjaan.csv (dfc_1)
```
Tujuan: Menggabungkan informasi keahlian yang diperlukan untuk setiap pekerjaan dengan deskripsi pekerjaan itu sendiri.
```
"""

# Penggabungan dataset dengan left join
dfc_1 = pd.merge(df_lk_clean, df_skill_pekerjaan, on='id_pekerjaan', how='left')
pd.set_option('display.max_columns', None)
dfc_1

dfc_1.info()

"""### industri_pekerjaan.csv dan industri.csv (dfc_2)
```
Tujuan: Memfokuskan analisis pada industri IT Services and IT Consulting.

```
"""

# Penggabungan dataset
dfc_2 = pd.merge(df_industri_pekerjaan, df_industri, on='id_industri')
dfc_2

dfc_2.info()

"""### dfc_1 dan perusahaan.csv (dfc_3)
```
Tujuan: Menambahkan informasi relevan tentang perusahaan yang menawarkan pekerjaan.

```
"""

# Penggabungan dataset dengan left join
dfc_3 = pd.merge(dfc_1, df_perusahaan_clean, on='id_perusahaan', how='left')
dfc_3

dfc_3.info()

"""### dfc_3 dan upah.csv (dfc_4_clear_once)
```
Tujuan: Memasukkan nilai gaji dari file upah
```
"""

# Penggabungan dataset dengan right join (kiri ikut data ke kanan)
dfc_4 = pd.merge(dfc_3, df_gaji_clean, on='id_pekerjaan', how='left')
dfc_4

dfc_4.info()

# Satukan kolom kolom gaji_maksimal_x	gaji_tengah	gaji_minimal_x	periode_pembayaran_x dengan kolom gaji_maksimal_y	gaji_median	gaji_minimal_y	periode_pembayaran_y
dfc_4['gaji_maksimal_x'] = dfc_4['gaji_maksimal_y']
dfc_4['gaji_tengah'] = dfc_4['gaji_median']
dfc_4['gaji_minimal_x'] = dfc_4['gaji_minimal_y']
dfc_4['periode_pembayaran_x'] = dfc_4['periode_pembayaran_y']

# Menghapus kolom yang tidak diperlukan
dfc_4.drop(['gaji_maksimal_y', 'gaji_median', 'gaji_minimal_y', 'periode_pembayaran_y'], axis=1, inplace=True)

dfc_4

# Mengganti nama kolom
dfc_4.rename(columns={
    'gaji_maksimal_x': 'gaji_maksimal',
    'gaji_tengah': 'gaji_median',
    'gaji_minimal_x': 'gaji_minimal',
    'periode_pembayaran_x': 'periode_pembayaran',
    'nama': 'nama_perusahaan'
}, inplace=True)

dfc_4

# Menghapus baris yang memiliki nilai kosong diketiga kolom gaji
dfc_4.dropna(how='all', subset=['gaji_maksimal', 'gaji_median', 'gaji_minimal'], inplace=True)

dfc_4

variasi_periode = dfc_4['periode_pembayaran'].unique()
print(variasi_periode)

# Menghapus baris dengan nilai 'ONCE' pada kolom 'periode_pembayaran'
dfc_4_clear_once = dfc_4[dfc_4['periode_pembayaran'] != 'ONCE']

# Menampilkan DataFrame setelah penghapusan baris
dfc_4_clear_once

variasi_periode = dfc_4_clear_once['periode_pembayaran'].unique()
print(variasi_periode)

# Featured Engineering : Kolom Baru untuk Gaji/Hour

# Fungsi untuk mengonversi gaji berdasarkan periode pembayaran
def konversi_gaji(gaji, periode):
    if periode == 'WEEKLY':
        return gaji / 40  # 40 jam per minggu
    elif periode == 'MONTHLY':
        return gaji / 196  # 196 jam per bulan
    elif periode == 'YEARLY':
        return gaji / 2080  # 2080 jam per tahun
    else:
        return gaji  # Untuk periode Hourly, tidak perlu dikonversi

# Mengonversi gaji maksimum, median, dan minimum
dfc_4_clear_once['gaji_maks_perjam'] = dfc_4_clear_once.apply(lambda x: konversi_gaji(x['gaji_maksimal'], x['periode_pembayaran']), axis=1)
dfc_4_clear_once['gaji_median_perjam'] = dfc_4_clear_once.apply(lambda x: konversi_gaji(x['gaji_median'], x['periode_pembayaran']), axis=1)
dfc_4_clear_once['gaji_min_perjam'] = dfc_4_clear_once.apply(lambda x: konversi_gaji(x['gaji_minimal'], x['periode_pembayaran']), axis=1)

# Menampilkan DataFrame yang telah diupdate
dfc_4_clear_once

"""### dfc_4_clear_once dan kemampuan.csv (dfc_5)"""

# Penggabungan dataset dengan left join (kanan ikut data ke kiri)

dfc_5 = pd.merge(dfc_4_clear_once, df_kemampuan, on='singkatan_kemampuan')
dfc_5

# Menghapus kolom singkatan kemampuan
dfc_5.drop(['singkatan_kemampuan'], axis=1, inplace=True)
dfc_5

dfc_5.info()

"""### df_it dan dfc_5 (dfc_final)
```
Tujuan: Untuk mengkerucutkan dataset ke industri IT Services and IT Consulting.

```
"""

dfc_sebelumfinal = pd.merge(dfc_2, dfc_5, on='id_pekerjaan', how='left')
dfc_sebelumfinal

dfc_final = dfc_sebelumfinal[dfc_sebelumfinal['nama_industri'] == 'IT Services and IT Consulting']
dfc_final

# Menghitung data duplikat pada salah satu kolom
print("Banyaknya data duplikat pada 'id_pekerjaan' yaitu", dfc_final.id_pekerjaan.duplicated().sum())

# Menghapus baris yang memiliki nilai kosong diketiga kolom gaji
dfc_final.dropna(how='all', subset=['gaji_maksimal', 'gaji_median', 'gaji_minimal'], inplace=True)

dfc_final

# Menghapus Kolom yang tidak digunakan
dfc_final_clean = ['id_industri', 'id_perusahaan', 'gaji_maksimal', 'gaji_median',	'gaji_minimal',	'periode_pembayaran', 'deskripsi_keterampilan']
dfc_final_clean = dfc_final.drop(columns=dfc_final_clean)

dfc_final_clean

dfc_final_clean.dropna(inplace=True)
dfc_final_clean

"""#EDA"""

from wordcloud import WordCloud

# Memanggil dataset kedalam projek
data_eda = dfc_final_clean
data_eda.head(5)

data_eda.info()

data_eda.describe().transpose()

data_eda.describe(include=object).transpose()

data_eda.shape

data_eda.isnull().sum()

data_eda.duplicated().sum()

data_clean = data_eda

data_clean['tingkat_pengalaman_terformat'] = data_clean['tingkat_pengalaman_terformat'].apply(lambda x: x.capitalize() if x == 'unknown' else x)
data_clean['jenis_pekerjaan'] = data_clean['jenis_pekerjaan'].replace({
    'FULL_TIME': 'Full-time',
    'CONTRACT': 'Contract',
    'PART_TIME': 'Part-time',
    'INTERNSHIP': 'Internship',
    'TEMPORARY': 'Temporary'})

data_clean

distribusi_jenis_pekerjaan = data_clean['jenis_pekerjaan'].value_counts()

plt.figure(figsize=(10, 6))
ax = distribusi_jenis_pekerjaan.plot(kind='bar', color='lightseagreen')
plt.title('Job Distribution by Work Type')
plt.xlabel('Work Type')
plt.ylabel('Number of Jobs')
plt.xticks(rotation=0)
plt.tight_layout()

for i in ax.containers:
    ax.bar_label(i, label_type='edge')
plt.show()

"""**Distribusi Jenis Pekerjaan**

Dari visualisasi diatas maka dapat disimpulkan distribusi tiap jenis pekerjaan ialah:


*   Full-time berjumlah 880 pekerjaan
*   Contract berjumlah 327 pekerjaan
*   Part-time berjumlah 21 pekerjaan
*   Internship berjumlah 7 pekerjaan
*   Temporary berjumlah 4 pekerjaan


"""

distribusi_tingkat_pengalaman = data_clean['tingkat_pengalaman_terformat'].value_counts()

plt.figure(figsize=(10, 6))
ax = distribusi_tingkat_pengalaman.plot(kind='bar', color='lightseagreen')
plt.title('Job Distribution by Experience Level')
plt.xlabel('Experience Level')
plt.ylabel('Number of Jobs')
plt.xticks(rotation=0)
plt.tight_layout()

for i in ax.containers:
    ax.bar_label(i, label_type='edge')
plt.show()

"""**Distribusi Tingkat Pengalaman**

Dari visualisasi diatas maka dapat disimpulkan distribusi tiap tingkat pengalaman ialah:


*   Mid-Senior level berjumlah 537 pekerjaan
*   Entry level berjumlah 147 pekerjaan
*   Associate berjumlah 106 pekerjaan
*   Director berjumlah 43 pekerjaan
*   Executive berjumlah 14 pekerjaan
*   Internship berjumlah 11 pekerjaan
*   Unknown berjumlah 381 pekerjaan


"""

job_titles_text = ' '.join(data_clean['judul'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(job_titles_text)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Most Job Title in the Industry of IT Services and IT Consulting')
plt.axis('off')
plt.tight_layout()
plt.show()

distribusi_pekerjaan = data_clean['judul'].value_counts()
print(distribusi_pekerjaan)

"""Analisis kemampuan yang paling menonjol"""

skill_name_text = ' '.join(data_clean['nama_kemampuan'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(skill_name_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Top Skill Requirements for the Industry of IT Services and IT Consulting')
plt.axis('off')
plt.tight_layout()
plt.show()

sns.set(style="whitegrid")

plt.figure(figsize=(12, 8))
sns.countplot(y='nama_kemampuan', hue='nama_kemampuan', data=data_clean,
              order=data_clean['nama_kemampuan'].value_counts().index,
              palette='bright', legend=False)
plt.title('Job Distribution by Skill in the Industry of IT Services and IT Consulting')
plt.xlabel('Count')
plt.ylabel('Skill')
plt.show()

distribusi_skill= data_clean['nama_kemampuan'].value_counts()
print(distribusi_skill)

data_numerik = data_clean.select_dtypes(include=[np.number])

correlation_matrix = data_numerik.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
categorical_columns = data_clean.select_dtypes(include='object').columns
data_clean[categorical_columns] = data_clean[categorical_columns].apply(lambda col: le.fit_transform(col.astype(str)))

data_check = data_clean.drop(columns=['nama_industri'])

correlation_matrix = data_check.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix')
plt.show()

"""# Modelling"""

from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, PolynomialFeatures, MaxAbsScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.compose import ColumnTransformer

data = dfc_final_clean
pd.set_option('display.max_columns', None)
data

# memilih fitur dan target
data1 = data[['nama_industri', 'judul', 'deskripsi', 'tingkat_pengalaman_terformat', 'jenis_pekerjaan', 'nama_perusahaan', 'nama_kemampuan', 'gaji_min_perjam', 'gaji_maks_perjam','gaji_median_perjam',]]
data = data1[['tingkat_pengalaman_terformat', 'jenis_pekerjaan', 'gaji_maks_perjam', 'gaji_median_perjam', 'gaji_min_perjam', 'nama_kemampuan', 'nama_perusahaan', 'judul']]
X1 = data.drop('gaji_maks_perjam', axis=1)
X2 = data.drop('gaji_median_perjam', axis=1)
X3 = data.drop('gaji_median_perjam', axis=1)
X4 = data1.drop('gaji_median_perjam', axis=1)
y1 = data['gaji_maks_perjam']
y2 = data['gaji_median_perjam']
y3 = data['gaji_median_perjam']
y4 = data1['gaji_median_perjam']

"""### Linear Regression"""

# inisialisasi scaler
lr = LinearRegression(copy_X=True,
                      fit_intercept=True,
                      n_jobs=None)
# membuat pipeline
pipeline = make_pipeline(StandardScaler(with_mean=False), lr)

# memisahkan antara data latih dan data test
X_train1, X_test1, y_train1, y_test1 = train_test_split(X3, y3, test_size=0.3, random_state=0)

# Fit the model
pipeline.fit(X_train1, y_train1)

# prediksi target pada dataset test
y_pred1 = pipeline.predict(X_test1)

# skor akurasi di dataset test
accuracy_test1 = r2_score(y_test1,y_pred1)
print('skor akurasi pada dataset test : ', round((accuracy_test1*100),3))

# Evaluasi model
mae_rl = mean_absolute_error(y_test1, y_pred1)
mse_rl = mean_squared_error(y_test1, y_pred1)
r2_rl = r2_score(y_test1, y_pred1)
rmse_rl = np.sqrt(mse_rl)

print(f"Evaluasi Model (Data Asli) Regresi Linear:")
print(f"Mean Absolute Error (MAE):", round(mae_rl, 3))
print(f"Mean Squared Error (MSE):", round(mse_rl, 3))
print(f"R-squared (R²):", round(r2_rl, 3))
print(f"Root Mean Squared Error (RMSE): ", round(rmse_rl, 3))

"""#### Regression Plot"""

# membuat scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test1, y=y_pred1, color='blue', label='Prediksi')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')

# garis plot warna merah
plt.plot(y_test1, y_test1, color='red', label='Garis Regresi')

plt.title('Actual vs Predicted Values')
plt.legend()
plt.show()

# Data evaluasi
evaluasi_data = {
    'Metric': ['MAE', 'MSE', 'R²', 'RMSE'],
    'Value': [mae_rl, mse_rl, r2_rl, rmse_rl]
}

# Membuat DataFrame dari data evaluasi
evaluasi_df = pd.DataFrame(evaluasi_data)

# Mengatur gaya visualisasi Seaborn
sns.set(style="whitegrid")

# Membuat bar plot untuk visualisasi evaluasi model
plt.figure(figsize=(10, 6))
ax = sns.barplot(x='Metric', y='Value', data=evaluasi_df, palette='viridis')

plt.title('Evaluasi Model (Data Asli)')
plt.ylabel('Value')
plt.xlabel('Metric')
plt.ylim(0, max(evaluasi_df['Value']) * 1.1)  # Untuk sedikit ruang di atas bar tertinggi

# Menampilkan nilai pada setiap bar
for container in ax.containers:
    ax.bar_label(container)

plt.show()

"""### Polynomial Regression"""

# Praproses data (OneHotEncoding untuk variabel kategorikal dengan handle_unknown='ignore')
categorical_features = ['judul', 'deskripsi', 'tingkat_pengalaman_terformat', 'jenis_pekerjaan', 'nama_perusahaan', 'nama_kemampuan', 'nama_industri']
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# memisahkan antara data latih dan data test
X_train2, X_test2, y_train2, y_test2 = train_test_split(X4, y4, test_size=0.2, random_state=42)

# Membuat pipeline dengan PolynomialFeatures dan LinearRegression
degree = 2
model = Pipeline([
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=degree)),
    #('scaler', MaxAbsScaler()),
    ('scaler', StandardScaler(with_mean=False)),  # Memastikan StandardScaler tidak mengurangi mean untuk data sparse
    ('regressor', LinearRegression())
])

# Melatih model
model.fit(X_train2, y_train2)

# Memprediksi dengan model
y_pred2 = model.predict(X_test2)

# skor akurasi di dataset test
accuracy_test2 = r2_score(y_test2,y_pred2)
print('skor akurasi pada dataset test : ', round((accuracy_test2*100),3))

# Evaluasi model
mae_poly = mean_absolute_error(y_test2, y_pred2)
mse_poly = mean_squared_error(y_test2, y_pred2)
r2_poly = r2_score(y_test2, y_pred2)
rmse_poly = np.sqrt(mse_rl)

print(f"Evaluasi Model (Data Asli) Regresi Linear:")
print(f"Mean Absolute Error (MAE):", round(mae_poly, 3))
print(f"Mean Squared Error (MSE):", round(mse_poly, 3))
print(f"R-squared (R²):", round(r2_poly, 3))
print(f"Root Mean Squared Error (RMSE): ", round(rmse_poly, 3))

"""#### Regression Plot"""

# Fit data asli dan prediksi dengan model regresi polinomial
poly = PolynomialFeatures(degree=2)
y_test2_poly = poly.fit_transform(np.array(y_test2).reshape(-1, 1))
model = LinearRegression().fit(y_test2_poly, y_pred2)

# Membuat array x dari nilai minimum sampai nilai maksimum y_test2
X_visual = np.linspace(y_test2.min(), y_test2.max(), 400).reshape(-1, 1)

# Membuat array y dengan model regresi polinomial
y_visual = model.predict(poly.transform(X_visual))

# Plot data asli vs prediksi
plt.scatter(y_test2, y_pred2, color='blue', label='Prediksi')
plt.plot(X_visual, y_visual, 'r--', lw=2, label='Garis Polinomial')
plt.xlabel('Nilai Asli')
plt.ylabel('Nilai Prediksi')
plt.title('Plot Regresi: Nilai Asli vs Nilai Prediksi')
plt.legend()
plt.show()

# Data evaluasi
evaluasi_data_poly = {
    'Metric': ['MAE', 'MSE', 'R²', 'RMSE'],
    'Value': [mae_poly, mse_poly, r2_poly, rmse_poly]
}

# Membuat DataFrame dari data evaluasi
evaluasi_df_poly = pd.DataFrame(evaluasi_data_poly)

# Mengatur gaya visualisasi Seaborn
sns.set(style="whitegrid")

# Membuat bar plot untuk visualisasi evaluasi model
plt.figure(figsize=(10, 6))
ar = sns.barplot(x='Metric', y='Value', data=evaluasi_data_poly, palette='viridis')

plt.title('Evaluasi Model (Data Asli)')
plt.ylabel('Value')
plt.xlabel('Metric')
plt.ylim(0, max(evaluasi_df_poly['Value']) * 1.1)  # Untuk sedikit ruang di atas bar tertinggi

# Menampilkan nilai pada setiap bar
for container in ar.containers:
    ar.bar_label(container)

plt.show()